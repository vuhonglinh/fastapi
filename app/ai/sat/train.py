import os, re, json, time, logging
import torch
from pathlib import Path
from datasets import load_dataset
from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments

# =====================
# CONFIG (FIX Cá»¨NG Táº M)
# =====================
BANK_ID = "A1234"
MODEL_FAMILY = "roberta-large"

DATA_PATH = "data/sat_questions_new.json"       
TAXONOMY_PATH = "data/subskill_label2id.json"   

MODEL_ROOT = Path("models") / BANK_ID / MODEL_FAMILY  
# BASE_MODEL_DIR = Path("models/hf/roberta-large")  
BASE_MODEL_DIR = "roberta-base"
    

logging.basicConfig(level=logging.CRITICAL)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def get_latest_version_dir(root: Path) -> Path:
    if not root.exists():
        return None
    versions = []
    for p in root.iterdir():
        if p.is_dir():
            m = re.fullmatch(r"v(\d+)", p.name)
            if m:
                versions.append((int(m.group(1)), p))
    if not versions:
        return None
    return sorted(versions, key=lambda x: x[0])[-1][1]

def next_version_dir(root: Path):
    root.mkdir(parents=True, exist_ok=True)
    versions = []
    for p in root.iterdir():
        if p.is_dir():
            m = re.fullmatch(r"v(\d+)", p.name)
            if m:
                versions.append(int(m.group(1)))
    n = (max(versions) + 1) if versions else 1
    v = f"v{n}"
    return v, root / v

# =====================
# LOAD TAXONOMY
# =====================
with open(TAXONOMY_PATH, "r", encoding="utf-8") as f:
    name2global = json.load(f)
global2name = {v: k for k, v in name2global.items()}

# =====================
# LOAD NEW DATA (JSON)
# =====================
ds = load_dataset("json", data_files=DATA_PATH)["train"]
ds = ds.train_test_split(test_size=0.1)

all_global = list(ds["train"]["label"]) + list(ds["test"]["label"])
all_global = [int(x) for x in all_global]

unknown = sorted({x for x in all_global if x not in global2name})
if unknown:
    raise ValueError(f"âŒ File data cÃ³ label chÆ°a cÃ³ trong taxonomy: {unknown}")

unique_global = sorted(set(all_global))
num_labels = len(unique_global)

# remap dense theo label xuáº¥t hiá»‡n trong file má»›i
orig2dense = {g: i for i, g in enumerate(unique_global)}
dense2orig = {i: g for g, i in orig2dense.items()}

id2label = {i: global2name[dense2orig[i]] for i in range(num_labels)}
label2id = {v: k for k, v in id2label.items()}

print(f"âœ… labels_in_new_file={num_labels} dense=0..{num_labels-1}")

# =====================
# CHá»ŒN MODEL Äá»‚ TRAIN TIáº¾P
# =====================
latest = get_latest_version_dir(MODEL_ROOT)
if latest is None:
    # chÆ°a cÃ³ model nÃ o -> train tá»« base
    load_model_dir = BASE_MODEL_DIR
    load_tok_dir = BASE_MODEL_DIR
    print("âš ï¸ ChÆ°a cÃ³ version nÃ o -> train tá»« base roberta-large")
else:
    load_model_dir = latest / "model"
    load_tok_dir = latest / "tokenizer"
    print(f"ðŸ” Continue training from: {latest.name}")

# NOTE: chá»‰ continue Ä‘Æ°á»£c náº¿u label-set KHÃ”NG Ä‘á»•i so vá»›i model cÅ©
# (vÃ¬ classifier head shape pháº£i khá»›p)
if latest is not None:
    remap_path = latest / "label_remap.json"
    if not remap_path.exists():
        raise FileNotFoundError(f"âŒ Missing label_remap.json in {latest}")
    with open(remap_path, "r", encoding="utf-8") as f:
        old_remap = json.load(f)
    old_labels = set(int(v) for v in old_remap["dense2orig"].values())
    new_labels = set(unique_global)
    if new_labels != old_labels:
        raise RuntimeError(
            "âŒ Label-set Ä‘Ã£ thay Ä‘á»•i so vá»›i model latest.\n"
            f"latest={sorted(old_labels)}\nnew={sorted(new_labels)}\n"
            "=> KhÃ´ng thá»ƒ train tiáº¿p. HÃ£y cháº¡y fine_tune.py (rebuild head) Ä‘á»ƒ publish version má»›i."
        )

# =====================
# LOAD MODEL + TOKENIZER
# =====================
model = RobertaForSequenceClassification.from_pretrained(
    str(load_model_dir),
    num_labels=num_labels,
    id2label=id2label,
    label2id=label2id,
).to(DEVICE)

tokenizer = RobertaTokenizer.from_pretrained(str(load_tok_dir))

def tokenize(batch):
    tok = tokenizer(
        batch["question"],
        padding="max_length",
        truncation=True,
        max_length=256,
    )
    tok["labels"] = [orig2dense[int(x)] for x in batch["label"]]
    return tok

tok_ds = ds.map(tokenize, batched=True, remove_columns=ds["train"].column_names)
tok_ds.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# =====================
# TRAIN
# =====================
VERSION, SAVE_DIR = next_version_dir(MODEL_ROOT)
SAVE_DIR.mkdir(parents=True, exist_ok=True)

args = TrainingArguments(
    output_dir=str(SAVE_DIR / "logs"),
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    warmup_steps=50,
    weight_decay=0.01,
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    fp16=torch.cuda.is_available(),
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tok_ds["train"],
    eval_dataset=tok_ds["test"],
)

trainer.train()

# =====================
# SAVE VERSION
# =====================
(model_dir := SAVE_DIR / "model").mkdir(parents=True, exist_ok=True)
(tok_dir := SAVE_DIR / "tokenizer").mkdir(parents=True, exist_ok=True)

model.save_pretrained(str(model_dir))
tokenizer.save_pretrained(str(tok_dir))

with open(SAVE_DIR / "label_remap.json", "w", encoding="utf-8") as f:
    json.dump(
        {
            "global_ids_in_this_version": unique_global,
            "orig2dense": {str(k): v for k, v in orig2dense.items()},
            "dense2orig": {str(k): v for k, v in dense2orig.items()},
        },
        f,
        ensure_ascii=False,
        indent=2,
    )

with open(SAVE_DIR / "meta.json", "w", encoding="utf-8") as f:
    json.dump(
        {
            "bank_id": BANK_ID,
            "model_family": MODEL_FAMILY,
            "version": VERSION,
            "continued_from": latest.name if latest else None,
            "trained_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "data_path": DATA_PATH,
            "num_labels": num_labels,
        },
        f,
        ensure_ascii=False,
        indent=2,
    )

print(f"âœ… Continue training completed â†’ saved to {SAVE_DIR}")
